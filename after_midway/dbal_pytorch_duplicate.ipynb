{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an implementation of the paper Deep Bayesian Active Learning with Image Data using PyTorch and modAL. \n",
    "\n",
    "modAL is an active learning framework for Python3, designed with modularity, flexibility and extensibility in mind. Built on top of scikit-learn, it allows you to rapidly create active learning workflows with nearly complete freedom. What is more, you can easily replace parts with your custom built solutions, allowing you to design novel algorithms with ease.\n",
    "\n",
    "Since modAL only supports sklearn models, we will also use [skorch](https://skorch.readthedocs.io/en/stable/), a scikit-learn compatible neural network library that wraps PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saumy\\Anaconda3\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: [WinError 126] The specified module could not be found\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import MNIST\n",
    "from skorch import NeuralNetClassifier\n",
    "from modAL.models import ActiveLearner\n",
    "\n",
    "# from VAE import VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### architecture of the network we will be using\n",
    "\n",
    "We will use the architecture described in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN(nn.Module):\n",
    "#     def __init__(self,):\n",
    "#         super(CNN, self).__init__()\n",
    "#         self.convs = nn.Sequential(\n",
    "#                                 nn.Conv2d(1,32,4),\n",
    "#                                 nn.ReLU(),\n",
    "#                                 nn.Conv2d(32,32,4),\n",
    "#                                 nn.ReLU(),\n",
    "#                                 nn.MaxPool2d(2),\n",
    "#                                 nn.Dropout(0.25)\n",
    "#         )\n",
    "#         self.fcs = nn.Sequential(\n",
    "#                                 nn.Linear(11*11*32,128),\n",
    "#                                 nn.ReLU(),\n",
    "#                                 nn.Dropout(0.5),\n",
    "#                                 nn.Linear(128,10),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = x\n",
    "#         out = self.convs(out)\n",
    "#         out = out.view(-1,11*11*32)\n",
    "#         out = self.fcs(out)\n",
    "#         return out\n",
    "\n",
    "class lenet(nn.Module):  \n",
    "    def __init__(self):\n",
    "        super(lenet, self).__init__()\n",
    "        self.input_height = input_height\n",
    "        self.input_width = input_width\n",
    "        self.input_dim = input_dim\n",
    "        self.class_num = 10\n",
    "\n",
    "        self.conv1 = nn.Conv2d(self.input_dim, 6, (5, 5), padding=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, (5, 5))\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, self.class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = MNIST('.', train=True, download=True, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "mnist_test  = MNIST('.', train=False,download=True, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "traindataloader = DataLoader(mnist_train, shuffle=True, batch_size=60000)\n",
    "testdataloader  = DataLoader(mnist_test , shuffle=True, batch_size=10000)\n",
    "X_train, y_train = next(iter(traindataloader))\n",
    "X_test , y_test  = next(iter(testdataloader))\n",
    "X_train, y_train = X_train.detach().cpu().numpy(), y_train.detach().cpu().numpy()\n",
    "X_test, y_test = X_test.detach().cpu().numpy(), y_test.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000, 1, 28, 28)\n",
    "X_test = X_test.reshape(10000, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initial labelled data\n",
    "We initialize the labelled set with 100 balanced randomly sampled examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_idx = np.array([],dtype=int)\n",
    "for i in range(10):\n",
    "    idx = np.random.choice(np.where(y_train==i)[0], size=10, replace=False)\n",
    "    initial_idx = np.concatenate((initial_idx, idx))\n",
    "\n",
    "X_initial = X_train[initial_idx]\n",
    "y_initial = y_train[initial_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initial unlabelled pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_pool = np.delete(X_train, initial_idx, axis=0)\n",
    "# y_pool = np.delete(y_train, initial_idx, axis=0)\n",
    "\n",
    "X_pool = np.copy(X_train)\n",
    "y_pool = np.copy(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform\n",
    "All the acquisition function we will use will be compared to the uniform acquisition function $\\mathbb{U}_{[0,1]}$ which will be our baseline that we would like to beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform(learner, X, n_instances=1):\n",
    "    query_idx = np.random.choice(range(len(X)), size=n_instances, replace=False)\n",
    "    return query_idx, X[query_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "Our first acquisition function is the entropy:\n",
    "$$ \\mathbb{H} = - \\sum_{c} p_c \\log(p_c)$$\n",
    "where $p_c$ is the probability predicted for class c. This is approximated by:\n",
    "\\begin{align}\n",
    "p_c &= \\frac{1}{T} \\sum_t p_{c}^{(t)} \n",
    "\\end{align}\n",
    "where $p_{c}^{t}$ is the probability predicted for class c at the t th feedforward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_entropy(learner, X, n_instances=1, T=100):\n",
    "    random_subset = np.random.choice(range(len(X)), size=2000, replace=False)\n",
    "    with torch.no_grad():\n",
    "        outputs = np.stack([torch.softmax(learner.estimator.forward(X[random_subset], training=True),dim=-1).cpu().numpy()\n",
    "                            for t in range(20)])\n",
    "    pc = outputs.mean(axis=0)\n",
    "    acquisition = (-pc*np.log(pc + 1e-10)).sum(axis=-1)\n",
    "    idx = (-acquisition).argsort()[:n_instances]\n",
    "    query_idx = random_subset[idx]\n",
    "    return query_idx, X[query_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bald(learner, X, n_instances=1, T=100):\n",
    "    random_subset = np.random.choice(range(len(X)), size=2000, replace=False)\n",
    "    with torch.no_grad():\n",
    "        outputs = np.stack([torch.softmax(learner.estimator.forward(X[random_subset], training=True),dim=-1).cpu().numpy()\n",
    "                            for t in range(20)])\n",
    "    pc = outputs.mean(axis=0)\n",
    "    H   = (-pc*np.log(pc + 1e-10)).sum(axis=-1)\n",
    "    E_H = - np.mean(np.sum(outputs * np.log(outputs + 1e-10), axis=-1), axis=0)  # [batch size]\n",
    "    acquisition = H - E_H\n",
    "    idx = (-acquisition).argsort()[:n_instances]\n",
    "    query_idx = random_subset[idx]\n",
    "    return query_idx, X[query_idx]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list(input_list, name):\n",
    "    with open(\"perf_lists_duplicate/\" + name, 'w') as f:\n",
    "        for val in input_list:\n",
    "            f.write(\"%s\\n\" % val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active Learning Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_learning_procedure_duplicate(query_strategy,\n",
    "                              X_test,\n",
    "                              y_test,\n",
    "                              X_pool,\n",
    "                              y_pool,\n",
    "                              X_initial,\n",
    "                              y_initial,\n",
    "                              estimator,\n",
    "                              n_queries=150,\n",
    "                              n_instances=100):\n",
    "    learner = ActiveLearner(estimator=estimator,\n",
    "                            X_training=X_initial,\n",
    "                            y_training=y_initial,\n",
    "                            query_strategy=query_strategy,\n",
    "                           )\n",
    "    perf_hist = [learner.score(X_test, y_test)]\n",
    "    X_rolling, y_rolling = np.copy(X_initial), np.copy(y_initial)\n",
    "    for index in range(n_queries):\n",
    "        query_idx, query_instance = learner.query(X_pool, n_instances)\n",
    "        \n",
    "        new_samples = np.concatenate((X_pool[query_idx], X_pool[query_idx]))\n",
    "        new_labels = np.concatenate((y_pool[query_idx], y_pool[query_idx]))\n",
    "        \n",
    "        X_rolling, y_rolling = np.concatenate((X_rolling, new_samples), axis=0), np.concatenate((y_rolling, new_labels))\n",
    "        learner.fit(X_rolling, y_rolling)\n",
    "        \n",
    "        X_pool = np.delete(X_pool, query_idx, axis=0)\n",
    "        y_pool = np.delete(y_pool, query_idx, axis=0)\n",
    "        model_accuracy = learner.score(X_test, y_test)\n",
    "        print('Accuracy after query {n}: {acc:0.4f}'.format(n=index + 1, acc=model_accuracy))\n",
    "        perf_hist.append(model_accuracy)\n",
    "    return perf_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after query 1: 0.6701\n",
      "Accuracy after query 2: 0.6910\n",
      "Accuracy after query 3: 0.7327\n",
      "Accuracy after query 4: 0.7157\n",
      "Accuracy after query 5: 0.6995\n",
      "Accuracy after query 6: 0.7421\n",
      "Accuracy after query 7: 0.7293\n",
      "Accuracy after query 8: 0.7230\n",
      "Accuracy after query 9: 0.7675\n",
      "Accuracy after query 10: 0.8022\n",
      "Accuracy after query 11: 0.8654\n",
      "Accuracy after query 12: 0.8456\n",
      "Accuracy after query 13: 0.8823\n",
      "Accuracy after query 14: 0.8660\n",
      "Accuracy after query 15: 0.9124\n",
      "Accuracy after query 16: 0.9023\n",
      "Accuracy after query 17: 0.8841\n",
      "Accuracy after query 18: 0.9086\n",
      "Accuracy after query 19: 0.9159\n",
      "Accuracy after query 20: 0.9206\n",
      "Accuracy after query 21: 0.9273\n",
      "Accuracy after query 22: 0.9378\n",
      "Accuracy after query 23: 0.9327\n",
      "Accuracy after query 24: 0.9357\n",
      "Accuracy after query 25: 0.9391\n",
      "Accuracy after query 26: 0.9288\n",
      "Accuracy after query 27: 0.9374\n",
      "Accuracy after query 28: 0.9444\n",
      "Accuracy after query 29: 0.9450\n",
      "Accuracy after query 30: 0.9470\n",
      "Accuracy after query 31: 0.9527\n",
      "Accuracy after query 32: 0.9535\n",
      "Accuracy after query 33: 0.9579\n",
      "Accuracy after query 34: 0.9580\n",
      "Accuracy after query 35: 0.9595\n",
      "Accuracy after query 36: 0.9588\n",
      "Accuracy after query 37: 0.9592\n",
      "Accuracy after query 38: 0.9610\n",
      "Accuracy after query 39: 0.9655\n",
      "Accuracy after query 40: 0.9601\n",
      "Accuracy after query 41: 0.9674\n",
      "Accuracy after query 42: 0.9664\n",
      "Accuracy after query 43: 0.9651\n",
      "Accuracy after query 44: 0.9652\n",
      "Accuracy after query 45: 0.9665\n",
      "Accuracy after query 46: 0.9667\n",
      "Accuracy after query 47: 0.9626\n",
      "Accuracy after query 48: 0.9652\n",
      "Accuracy after query 49: 0.9667\n",
      "Accuracy after query 50: 0.9690\n",
      "Accuracy after query 51: 0.9671\n",
      "Accuracy after query 52: 0.9696\n",
      "Accuracy after query 53: 0.9666\n",
      "Accuracy after query 54: 0.9694\n",
      "Accuracy after query 55: 0.9715\n",
      "Accuracy after query 56: 0.9727\n",
      "Accuracy after query 57: 0.9719\n",
      "Accuracy after query 58: 0.9711\n",
      "Accuracy after query 59: 0.9709\n",
      "Accuracy after query 60: 0.9718\n",
      "Accuracy after query 61: 0.9733\n",
      "Accuracy after query 62: 0.9703\n",
      "Accuracy after query 63: 0.9694\n",
      "Accuracy after query 64: 0.9730\n",
      "Accuracy after query 65: 0.9730\n",
      "Accuracy after query 66: 0.9706\n",
      "Accuracy after query 67: 0.9722\n",
      "Accuracy after query 68: 0.9685\n",
      "Accuracy after query 69: 0.9761\n",
      "Accuracy after query 70: 0.9720\n",
      "Accuracy after query 71: 0.9703\n",
      "Accuracy after query 72: 0.9735\n",
      "Accuracy after query 73: 0.9757\n",
      "Accuracy after query 74: 0.9730\n",
      "Accuracy after query 75: 0.9758\n",
      "Accuracy after query 76: 0.9715\n",
      "Accuracy after query 77: 0.9770\n",
      "Accuracy after query 78: 0.9754\n",
      "Accuracy after query 79: 0.9756\n",
      "Accuracy after query 80: 0.9770\n",
      "Accuracy after query 81: 0.9757\n",
      "Accuracy after query 82: 0.9781\n",
      "Accuracy after query 83: 0.9780\n",
      "Accuracy after query 84: 0.9790\n",
      "Accuracy after query 85: 0.9808\n",
      "Accuracy after query 86: 0.9777\n",
      "Accuracy after query 87: 0.9789\n",
      "Accuracy after query 88: 0.9777\n",
      "Accuracy after query 89: 0.9801\n",
      "Accuracy after query 90: 0.9790\n",
      "Accuracy after query 91: 0.9813\n",
      "Accuracy after query 92: 0.9805\n",
      "Accuracy after query 93: 0.9814\n",
      "Accuracy after query 94: 0.9814\n",
      "Accuracy after query 95: 0.9820\n",
      "Accuracy after query 96: 0.9819\n",
      "Accuracy after query 97: 0.9800\n",
      "Accuracy after query 98: 0.9790\n",
      "Accuracy after query 99: 0.9821\n",
      "Accuracy after query 100: 0.9817\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "estimator = NeuralNetClassifier(lenet,\n",
    "                                max_epochs=50,\n",
    "                                batch_size=100,\n",
    "                                lr=1.0,\n",
    "                                optimizer=torch.optim.Adadelta,\n",
    "                                optimizer__rho=0.9,\n",
    "                                optimizer__eps=1e-6,\n",
    "                                criterion=torch.nn.CrossEntropyLoss,\n",
    "                                train_split=None,\n",
    "                                verbose=0,\n",
    "                                device=device)\n",
    "bald_duplicate_perf_hist = active_learning_procedure_duplicate(bald,\n",
    "                                           X_test,\n",
    "                                           y_test,\n",
    "                                           X_pool,\n",
    "                                           y_pool,\n",
    "                                           X_initial,\n",
    "                                           y_initial,\n",
    "                                           estimator,\n",
    "                                            n_instances=100)\n",
    "save_list(bald_duplicate_perf_hist, \"bald_duplicate_perf_hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after query 1: 0.6363\n",
      "Accuracy after query 2: 0.6767\n",
      "Accuracy after query 3: 0.6967\n",
      "Accuracy after query 4: 0.7257\n",
      "Accuracy after query 5: 0.7415\n",
      "Accuracy after query 6: 0.6868\n",
      "Accuracy after query 7: 0.6856\n",
      "Accuracy after query 8: 0.6616\n",
      "Accuracy after query 9: 0.7375\n",
      "Accuracy after query 10: 0.7680\n",
      "Accuracy after query 11: 0.8223\n",
      "Accuracy after query 12: 0.7988\n",
      "Accuracy after query 13: 0.8309\n",
      "Accuracy after query 14: 0.8460\n",
      "Accuracy after query 15: 0.8537\n",
      "Accuracy after query 16: 0.8600\n",
      "Accuracy after query 17: 0.8706\n",
      "Accuracy after query 18: 0.8858\n",
      "Accuracy after query 19: 0.8323\n",
      "Accuracy after query 20: 0.8938\n",
      "Accuracy after query 21: 0.9062\n",
      "Accuracy after query 22: 0.8900\n",
      "Accuracy after query 23: 0.9029\n",
      "Accuracy after query 24: 0.9042\n",
      "Accuracy after query 25: 0.9133\n",
      "Accuracy after query 26: 0.9354\n",
      "Accuracy after query 27: 0.9323\n",
      "Accuracy after query 28: 0.9316\n",
      "Accuracy after query 29: 0.9329\n",
      "Accuracy after query 30: 0.9407\n",
      "Accuracy after query 31: 0.9354\n",
      "Accuracy after query 32: 0.9331\n",
      "Accuracy after query 33: 0.9494\n",
      "Accuracy after query 34: 0.9362\n",
      "Accuracy after query 35: 0.9383\n",
      "Accuracy after query 36: 0.9465\n",
      "Accuracy after query 37: 0.9479\n",
      "Accuracy after query 38: 0.9529\n",
      "Accuracy after query 39: 0.9491\n",
      "Accuracy after query 40: 0.9579\n",
      "Accuracy after query 41: 0.9565\n",
      "Accuracy after query 42: 0.9557\n",
      "Accuracy after query 43: 0.9586\n",
      "Accuracy after query 44: 0.9571\n",
      "Accuracy after query 45: 0.9584\n",
      "Accuracy after query 46: 0.9601\n",
      "Accuracy after query 47: 0.9651\n",
      "Accuracy after query 48: 0.9618\n",
      "Accuracy after query 49: 0.9583\n",
      "Accuracy after query 50: 0.9656\n",
      "Accuracy after query 51: 0.9635\n",
      "Accuracy after query 52: 0.9709\n",
      "Accuracy after query 53: 0.9668\n",
      "Accuracy after query 54: 0.9700\n",
      "Accuracy after query 55: 0.9623\n",
      "Accuracy after query 56: 0.9734\n",
      "Accuracy after query 57: 0.9684\n",
      "Accuracy after query 58: 0.9708\n",
      "Accuracy after query 59: 0.9751\n",
      "Accuracy after query 60: 0.9696\n",
      "Accuracy after query 61: 0.9746\n",
      "Accuracy after query 62: 0.9639\n",
      "Accuracy after query 63: 0.9727\n",
      "Accuracy after query 64: 0.9707\n",
      "Accuracy after query 65: 0.9694\n",
      "Accuracy after query 66: 0.9736\n",
      "Accuracy after query 67: 0.9739\n",
      "Accuracy after query 68: 0.9717\n",
      "Accuracy after query 69: 0.9723\n",
      "Accuracy after query 70: 0.9763\n",
      "Accuracy after query 71: 0.9752\n",
      "Accuracy after query 72: 0.9750\n",
      "Accuracy after query 73: 0.9764\n",
      "Accuracy after query 74: 0.9762\n",
      "Accuracy after query 75: 0.9780\n",
      "Accuracy after query 76: 0.9778\n",
      "Accuracy after query 77: 0.9771\n",
      "Accuracy after query 78: 0.9799\n",
      "Accuracy after query 79: 0.9766\n",
      "Accuracy after query 80: 0.9739\n",
      "Accuracy after query 81: 0.9796\n",
      "Accuracy after query 82: 0.9773\n",
      "Accuracy after query 83: 0.9782\n",
      "Accuracy after query 84: 0.9806\n",
      "Accuracy after query 85: 0.9779\n",
      "Accuracy after query 86: 0.9802\n",
      "Accuracy after query 87: 0.9798\n",
      "Accuracy after query 88: 0.9833\n",
      "Accuracy after query 89: 0.9808\n",
      "Accuracy after query 90: 0.9816\n",
      "Accuracy after query 91: 0.9803\n",
      "Accuracy after query 92: 0.9800\n",
      "Accuracy after query 93: 0.9776\n",
      "Accuracy after query 94: 0.9811\n",
      "Accuracy after query 95: 0.9818\n",
      "Accuracy after query 96: 0.9814\n",
      "Accuracy after query 97: 0.9841\n",
      "Accuracy after query 98: 0.9820\n",
      "Accuracy after query 99: 0.9848\n",
      "Accuracy after query 100: 0.9849\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "estimator = NeuralNetClassifier(lenet,\n",
    "                                max_epochs=50,\n",
    "                                batch_size=100,\n",
    "                                lr=1.0,\n",
    "                                optimizer=torch.optim.Adadelta,\n",
    "                                optimizer__rho=0.9,\n",
    "                                optimizer__eps=1e-6,\n",
    "                                criterion=torch.nn.CrossEntropyLoss,\n",
    "                                train_split=None,\n",
    "                                verbose=0,\n",
    "                                device=device)\n",
    "entropy_duplicate_perf_hist = active_learning_procedure_duplicate(max_entropy,\n",
    "                                           X_test,\n",
    "                                           y_test,\n",
    "                                           X_pool,\n",
    "                                           y_pool,\n",
    "                                           X_initial,\n",
    "                                           y_initial,\n",
    "                                           estimator,\n",
    "                                            n_instances=100)\n",
    "save_list(entropy_duplicate_perf_hist, \"entropy_duplicate_perf_hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "estimator = NeuralNetClassifier(lenet,\n",
    "                                max_epochs=50,\n",
    "                                batch_size=100,\n",
    "                                lr=1.0,\n",
    "                                optimizer=torch.optim.Adadelta,\n",
    "                                optimizer__rho=0.9,\n",
    "                                optimizer__eps=1e-6,\n",
    "                                criterion=torch.nn.CrossEntropyLoss,\n",
    "                                train_split=None,\n",
    "                                verbose=0,\n",
    "                                device=device)\n",
    "uniform_duplicate_perf_hist = active_learning_procedure_duplicate(uniform,\n",
    "                                           X_test,\n",
    "                                           y_test,\n",
    "                                           X_pool,\n",
    "                                           y_pool,\n",
    "                                           X_initial,\n",
    "                                           y_initial,\n",
    "                                           estimator,\n",
    "                                            n_instances=100)\n",
    "save_list(uniform_duplicate_perf_hist, \"uniform_duplicate_perf_hist\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
