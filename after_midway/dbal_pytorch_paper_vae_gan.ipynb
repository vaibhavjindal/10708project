{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an implementation of the paper Deep Bayesian Active Learning with Image Data using PyTorch and modAL. \n",
    "\n",
    "modAL is an active learning framework for Python3, designed with modularity, flexibility and extensibility in mind. Built on top of scikit-learn, it allows you to rapidly create active learning workflows with nearly complete freedom. What is more, you can easily replace parts with your custom built solutions, allowing you to design novel algorithms with ease.\n",
    "\n",
    "Since modAL only supports sklearn models, we will also use [skorch](https://skorch.readthedocs.io/en/stable/), a scikit-learn compatible neural network library that wraps PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import MNIST\n",
    "from skorch import NeuralNetClassifier\n",
    "from modAL.models import ActiveLearner\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "# from VAE import VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### architecture of the network we will be using\n",
    "\n",
    "We will use the architecture described in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.tensor([0.1307], dtype=torch.float32)\n",
    "std = torch.tensor([0.3081], dtype=torch.float32)\n",
    "\n",
    "normalize = transforms.Normalize(mean.tolist(), std.tolist())\n",
    "unnormalize = transforms.Normalize((-mean / std).tolist(), (1.0 / std).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN(nn.Module):\n",
    "#     def __init__(self,):\n",
    "#         super(CNN, self).__init__()\n",
    "#         self.convs = nn.Sequential(\n",
    "#                                 nn.Conv2d(1,32,4),\n",
    "#                                 nn.ReLU(),\n",
    "#                                 nn.Conv2d(32,32,4),\n",
    "#                                 nn.ReLU(),\n",
    "#                                 nn.MaxPool2d(2),\n",
    "#                                 nn.Dropout(0.25)\n",
    "#         )\n",
    "#         self.fcs = nn.Sequential(\n",
    "#                                 nn.Linear(11*11*32,128),\n",
    "#                                 nn.ReLU(),\n",
    "#                                 nn.Dropout(0.5),\n",
    "#                                 nn.Linear(128,10),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = x\n",
    "#         out = self.convs(out)\n",
    "#         out = out.view(-1,11*11*32)\n",
    "#         out = self.fcs(out)\n",
    "#         return out\n",
    "input_dim, input_height, input_width = 1, 28, 28\n",
    "class lenet(nn.Module):  \n",
    "    def __init__(self):\n",
    "        super(lenet, self).__init__()\n",
    "        self.input_height = input_height\n",
    "        self.input_width = input_width\n",
    "        self.input_dim = input_dim\n",
    "        self.class_num = 10\n",
    "\n",
    "        self.conv1 = nn.Conv2d(self.input_dim, 6, (5, 5), padding=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, (5, 5))\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, self.class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = MNIST('.', train=True, download=True, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "mnist_test  = MNIST('.', train=False,download=True, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "traindataloader = DataLoader(mnist_train, shuffle=True, batch_size=60000)\n",
    "testdataloader  = DataLoader(mnist_test , shuffle=True, batch_size=10000)\n",
    "X_train, y_train = next(iter(traindataloader))\n",
    "X_test , y_test  = next(iter(testdataloader))\n",
    "X_train, y_train = X_train.detach().cpu().numpy(), y_train.detach().cpu().numpy()\n",
    "X_test, y_test = X_test.detach().cpu().numpy(), y_test.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000, 1, 28, 28)\n",
    "X_test = X_test.reshape(10000, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initial labelled data\n",
    "We initialize the labelled set with 100 balanced randomly sampled examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_idx = np.array([],dtype=int)\n",
    "for i in range(10):\n",
    "    idx = np.random.choice(np.where(y_train==i)[0], size=10, replace=False)\n",
    "    initial_idx = np.concatenate((initial_idx, idx))\n",
    "\n",
    "X_initial = X_train[initial_idx]\n",
    "y_initial = y_train[initial_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initial unlabelled pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_pool = np.delete(X_train, initial_idx, axis=0)\n",
    "# y_pool = np.delete(y_train, initial_idx, axis=0)\n",
    "\n",
    "X_pool = np.copy(X_train)\n",
    "y_pool = np.copy(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform\n",
    "All the acquisition function we will use will be compared to the uniform acquisition function $\\mathbb{U}_{[0,1]}$ which will be our baseline that we would like to beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform(learner, X, n_instances=1):\n",
    "    query_idx = np.random.choice(range(len(X)), size=n_instances, replace=False)\n",
    "    return query_idx, X[query_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "Our first acquisition function is the entropy:\n",
    "$$ \\mathbb{H} = - \\sum_{c} p_c \\log(p_c)$$\n",
    "where $p_c$ is the probability predicted for class c. This is approximated by:\n",
    "\\begin{align}\n",
    "p_c &= \\frac{1}{T} \\sum_t p_{c}^{(t)} \n",
    "\\end{align}\n",
    "where $p_{c}^{t}$ is the probability predicted for class c at the t th feedforward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_entropy(learner, X, n_instances=1, T=100):\n",
    "    random_subset = np.random.choice(range(len(X)), size=2000, replace=False)\n",
    "    with torch.no_grad():\n",
    "        outputs = np.stack([torch.softmax(learner.estimator.forward(X[random_subset], training=True),dim=-1).cpu().numpy()\n",
    "                            for t in range(20)])\n",
    "    pc = outputs.mean(axis=0)\n",
    "    acquisition = (-pc*np.log(pc + 1e-10)).sum(axis=-1)\n",
    "    idx = (-acquisition).argsort()[:n_instances]\n",
    "    query_idx = random_subset[idx]\n",
    "    return query_idx, X[query_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bald(learner, X, n_instances=1, T=100):\n",
    "    random_subset = np.random.choice(range(len(X)), size=2000, replace=False)\n",
    "    with torch.no_grad():\n",
    "        outputs = np.stack([torch.softmax(learner.estimator.forward(X[random_subset], training=True),dim=-1).cpu().numpy()\n",
    "                            for t in range(20)])\n",
    "    pc = outputs.mean(axis=0)\n",
    "    H   = (-pc*np.log(pc + 1e-10)).sum(axis=-1)\n",
    "    E_H = - np.mean(np.sum(outputs * np.log(outputs + 1e-10), axis=-1), axis=0)  # [batch size]\n",
    "    acquisition = H - E_H\n",
    "    idx = (-acquisition).argsort()[:n_instances]\n",
    "    query_idx = random_subset[idx]\n",
    "    return query_idx, X[query_idx]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list(input_list, name):\n",
    "    with open(\"perf_lists_vaegan/\" + name, 'w') as f:\n",
    "        for val in input_list:\n",
    "            f.write(\"%s\\n\" % val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active Learning Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_debug, y_debug = None, None\n",
    "def active_learning_procedure_generative(query_strategy,\n",
    "                              X_test,\n",
    "                              y_test,\n",
    "                              X_pool,\n",
    "                              y_pool,\n",
    "                              X_initial,\n",
    "                              y_initial,\n",
    "                              estimator,\n",
    "                              n_queries=150,\n",
    "                              n_instances=10):\n",
    "    \n",
    "    enc  = torch.load('./enc.pt', map_location=torch.device('cuda'))\n",
    "    dec  = torch.load('./dec.pt', map_location=torch.device('cuda'))\n",
    "    disc = torch.load('./disc.pt', map_location=torch.device('cuda'))\n",
    "    \n",
    "    def sample(mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        rand_z_score = torch.randn_like(std)\n",
    "        return mu + rand_z_score*std\n",
    "    \n",
    "    def vaeGanNewSampleGenerator(images):\n",
    "        images = Variable(images)\n",
    "        mu, logvar = enc(images)\n",
    "        z = sample(mu, logvar)\n",
    "        reconstructions = dec(z)\n",
    "        reconstructions = reconstructions.reshape(-1, 1, 28, 28)\n",
    "        return reconstructions\n",
    "\n",
    "    learner = ActiveLearner(estimator=estimator,\n",
    "                            X_training=X_initial,\n",
    "                            y_training=y_initial,\n",
    "                            query_strategy=query_strategy,\n",
    "                           )\n",
    "    perf_hist = [learner.score(X_test, y_test)]\n",
    "    X_rolling, y_rolling = np.copy(X_initial), np.copy(y_initial)\n",
    "    for index in range(n_queries):\n",
    "        query_idx, query_instance = learner.query(X_pool, n_instances)\n",
    "        \n",
    "        new_samples = vaeGanNewSampleGenerator(unnormalize(torch.tensor(X_pool[query_idx])).to(torch.device(\"cuda\")))\n",
    "        new_samples = normalize(new_samples)\n",
    "        new_samples = new_samples.detach().cpu().numpy()\n",
    "        new_samples = np.concatenate((new_samples, X_pool[query_idx]))\n",
    "        new_labels = np.concatenate((y_pool[query_idx], y_pool[query_idx]))\n",
    "        \n",
    "        X_rolling, y_rolling = np.concatenate((X_rolling, new_samples), axis=0), np.concatenate((y_rolling, new_labels))\n",
    "        try:\n",
    "            learner.fit(X_rolling, y_rolling)\n",
    "        except:\n",
    "            X_debug, y_debug = X_rolling, y_rolling\n",
    "        X_pool = np.delete(X_pool, query_idx, axis=0)\n",
    "        y_pool = np.delete(y_pool, query_idx, axis=0)\n",
    "        model_accuracy = learner.score(X_test, y_test)\n",
    "        print('Accuracy after query {n}: {acc:0.4f}'.format(n=index + 1, acc=model_accuracy))\n",
    "        perf_hist.append(model_accuracy)\n",
    "    return perf_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.activation.Sigmoid' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after query 1: 0.7744\n",
      "Accuracy after query 2: 0.8547\n",
      "Accuracy after query 3: 0.8630\n",
      "Accuracy after query 4: 0.8809\n",
      "Accuracy after query 5: 0.8980\n",
      "Accuracy after query 6: 0.9129\n",
      "Accuracy after query 7: 0.9013\n",
      "Accuracy after query 8: 0.9181\n",
      "Accuracy after query 9: 0.9346\n",
      "Accuracy after query 10: 0.9380\n",
      "Accuracy after query 11: 0.9269\n",
      "Accuracy after query 12: 0.9264\n",
      "Accuracy after query 13: 0.9334\n",
      "Accuracy after query 14: 0.9318\n",
      "Accuracy after query 15: 0.9314\n",
      "Accuracy after query 16: 0.9331\n",
      "Accuracy after query 17: 0.9226\n",
      "Accuracy after query 18: 0.9404\n",
      "Accuracy after query 19: 0.9490\n",
      "Accuracy after query 20: 0.9403\n",
      "Accuracy after query 21: 0.9473\n",
      "Accuracy after query 22: 0.9336\n",
      "Accuracy after query 23: 0.9422\n",
      "Accuracy after query 24: 0.9393\n",
      "Accuracy after query 25: 0.9461\n",
      "Accuracy after query 26: 0.9396\n",
      "Accuracy after query 27: 0.9553\n",
      "Accuracy after query 28: 0.9432\n",
      "Accuracy after query 29: 0.9480\n",
      "Accuracy after query 30: 0.9365\n",
      "Accuracy after query 31: 0.9484\n",
      "Accuracy after query 32: 0.9434\n",
      "Accuracy after query 33: 0.9463\n",
      "Accuracy after query 34: 0.9496\n",
      "Accuracy after query 35: 0.9199\n",
      "Accuracy after query 36: 0.9286\n",
      "Accuracy after query 37: 0.9464\n",
      "Accuracy after query 38: 0.9490\n",
      "Accuracy after query 39: 0.9394\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "estimator = NeuralNetClassifier(lenet,\n",
    "                                max_epochs=50,\n",
    "                                batch_size=100,\n",
    "                                lr=1.0,\n",
    "                                optimizer=torch.optim.Adadelta,\n",
    "                                optimizer__rho=0.9,\n",
    "                                optimizer__eps=1e-6,\n",
    "                                criterion=torch.nn.CrossEntropyLoss,\n",
    "                                train_split=None,\n",
    "                                verbose=0,\n",
    "                                device=device)\n",
    "uniform_gan_perf_hist = active_learning_procedure_generative(uniform,\n",
    "                                           X_test,\n",
    "                                           y_test,\n",
    "                                           X_pool,\n",
    "                                           y_pool,\n",
    "                                           X_initial,\n",
    "                                           y_initial,\n",
    "                                           estimator,\n",
    "                                            n_instances=100)\n",
    "save_list(uniform_gan_perf_hist, \"uniform_gan_perf_hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "estimator = NeuralNetClassifier(lenet,\n",
    "                                max_epochs=50,\n",
    "                                batch_size=100,\n",
    "                                lr=1.0,\n",
    "                                optimizer=torch.optim.Adadelta,\n",
    "                                optimizer__rho=0.9,\n",
    "                                optimizer__eps=1e-6,\n",
    "                                criterion=torch.nn.CrossEntropyLoss,\n",
    "                                train_split=None,\n",
    "                                verbose=0,\n",
    "                                device=device)\n",
    "bald_gan_perf_hist = active_learning_procedure_generative(bald,\n",
    "                                           X_test,\n",
    "                                           y_test,\n",
    "                                           X_pool,\n",
    "                                           y_pool,\n",
    "                                           X_initial,\n",
    "                                           y_initial,\n",
    "                                           estimator,\n",
    "                                        n_instances=100)\n",
    "save_list(bald_gan_perf_hist, \"bald_gan_perf_hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "estimator = NeuralNetClassifier(lenet,\n",
    "                                max_epochs=50,\n",
    "                                batch_size=100,\n",
    "                                lr=1.0,\n",
    "                                optimizer=torch.optim.Adadelta,\n",
    "                                optimizer__rho=0.9,\n",
    "                                optimizer__eps=1e-6,\n",
    "                                criterion=torch.nn.CrossEntropyLoss,\n",
    "                                train_split=None,\n",
    "                                verbose=0,\n",
    "                                device=device)\n",
    "entropy_gan_perf_hist = active_learning_procedure_generative(max_entropy,\n",
    "                                           X_test,\n",
    "                                           y_test,\n",
    "                                           X_pool,\n",
    "                                           y_pool,\n",
    "                                           X_initial,\n",
    "                                           y_initial,\n",
    "                                           estimator,\n",
    "                                            n_instances=100)\n",
    "save_list(entropy_gan_perf_hist, \"entropy_gan_perf_hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p38)",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
