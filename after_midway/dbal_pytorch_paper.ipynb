{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an implementation of the paper Deep Bayesian Active Learning with Image Data using PyTorch and modAL. \n",
    "\n",
    "modAL is an active learning framework for Python3, designed with modularity, flexibility and extensibility in mind. Built on top of scikit-learn, it allows you to rapidly create active learning workflows with nearly complete freedom. What is more, you can easily replace parts with your custom built solutions, allowing you to design novel algorithms with ease.\n",
    "\n",
    "Since modAL only supports sklearn models, we will also use [skorch](https://skorch.readthedocs.io/en/stable/), a scikit-learn compatible neural network library that wraps PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import MNIST\n",
    "from skorch import NeuralNetClassifier\n",
    "from modAL.models import ActiveLearner\n",
    "# from torchvision.models import resnet18\n",
    "# from torchvision import models\n",
    "from torchvision import transforms\n",
    "# from VAE import VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### architecture of the network we will be using\n",
    "\n",
    "We will use the architecture described in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN(nn.Module):\n",
    "#     def __init__(self,):\n",
    "#         super(CNN, self).__init__()\n",
    "#         self.convs = nn.Sequential(\n",
    "#                                 nn.Conv2d(1,32,4),\n",
    "#                                 nn.ReLU(),\n",
    "#                                 nn.Conv2d(32,32,4),\n",
    "#                                 nn.ReLU(),\n",
    "#                                 nn.MaxPool2d(2),\n",
    "#                                 nn.Dropout(0.25)\n",
    "#         )\n",
    "#         self.fcs = nn.Sequential(\n",
    "#                                 nn.Linear(11*11*32,128),\n",
    "#                                 nn.ReLU(),\n",
    "#                                 nn.Dropout(0.5),\n",
    "#                                 nn.Linear(128,10),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = x\n",
    "#         out = self.convs(out)\n",
    "#         out = out.view(-1,11*11*32)\n",
    "#         out = self.fcs(out)\n",
    "#         return out\n",
    "input_dim, input_height, input_width = 1, 28, 28\n",
    "class lenet(nn.Module):  \n",
    "    def __init__(self):\n",
    "        super(lenet, self).__init__()\n",
    "        self.input_height = input_height\n",
    "        self.input_width = input_width\n",
    "        self.input_dim = input_dim\n",
    "        self.class_num = 10\n",
    "\n",
    "        self.conv1 = nn.Conv2d(self.input_dim, 6, (5, 5), padding=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, (5, 5))\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, self.class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "# class ResNetMNIST(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         # define model and loss\n",
    "#         self.model = models.resnet18(num_classes=10, pretrained=False)\n",
    "#         self.model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "#         self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# #   @auto_move_data # this decorator automatically handles moving your tensors to GPU if required\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ResNetMNIST()\n",
    "# for param in model.parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = MNIST('.', train=True, download=True, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "mnist_test  = MNIST('.', train=False,download=True, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "traindataloader = DataLoader(mnist_train, shuffle=True, batch_size=60000)\n",
    "testdataloader  = DataLoader(mnist_test , shuffle=True, batch_size=10000)\n",
    "X_train, y_train = next(iter(traindataloader))\n",
    "X_test , y_test  = next(iter(testdataloader))\n",
    "X_train, y_train = X_train.detach().cpu().numpy(), y_train.detach().cpu().numpy()\n",
    "X_test, y_test = X_test.detach().cpu().numpy(), y_test.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000, 1, 28, 28)\n",
    "X_test = X_test.reshape(10000, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initial labelled data\n",
    "We initialize the labelled set with 100 balanced randomly sampled examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_idx = np.array([],dtype=int)\n",
    "for i in range(10):\n",
    "    idx = np.random.choice(np.where(y_train==i)[0], size=10, replace=False)\n",
    "    initial_idx = np.concatenate((initial_idx, idx))\n",
    "\n",
    "X_initial = X_train[initial_idx]\n",
    "y_initial = y_train[initial_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initial unlabelled pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_pool = np.delete(X_train, initial_idx, axis=0)\n",
    "# y_pool = np.delete(y_train, initial_idx, axis=0)\n",
    "X_pool = np.copy(X_train)\n",
    "y_pool = np.copy(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform\n",
    "All the acquisition function we will use will be compared to the uniform acquisition function $\\mathbb{U}_{[0,1]}$ which will be our baseline that we would like to beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform(learner, X, n_instances=1):\n",
    "    query_idx = np.random.choice(range(len(X)), size=n_instances, replace=False)\n",
    "    return query_idx, X[query_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "Our first acquisition function is the entropy:\n",
    "$$ \\mathbb{H} = - \\sum_{c} p_c \\log(p_c)$$\n",
    "where $p_c$ is the probability predicted for class c. This is approximated by:\n",
    "\\begin{align}\n",
    "p_c &= \\frac{1}{T} \\sum_t p_{c}^{(t)} \n",
    "\\end{align}\n",
    "where $p_{c}^{t}$ is the probability predicted for class c at the t th feedforward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_entropy(learner, X, n_instances=1, T=100):\n",
    "    random_subset = np.random.choice(range(len(X)), size=2000, replace=False)\n",
    "    with torch.no_grad():\n",
    "        outputs = np.stack([torch.softmax(learner.estimator.forward(X[random_subset], training=True),dim=-1).cpu().numpy()\n",
    "                            for t in range(20)])\n",
    "    pc = outputs.mean(axis=0)\n",
    "    acquisition = (-pc*np.log(pc + 1e-10)).sum(axis=-1)\n",
    "    idx = (-acquisition).argsort()[:n_instances]\n",
    "    query_idx = random_subset[idx]\n",
    "    return query_idx, X[query_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bald(learner, X, n_instances=1, T=100):\n",
    "    random_subset = np.random.choice(range(len(X)), size=2000, replace=False)\n",
    "    with torch.no_grad():\n",
    "        outputs = np.stack([torch.softmax(learner.estimator.forward(X[random_subset], training=True),dim=-1).cpu().numpy()\n",
    "                            for t in range(20)])\n",
    "    pc = outputs.mean(axis=0)\n",
    "    H   = (-pc*np.log(pc + 1e-10)).sum(axis=-1)\n",
    "    E_H = - np.mean(np.sum(outputs * np.log(outputs + 1e-10), axis=-1), axis=0)  # [batch size]\n",
    "    acquisition = H - E_H\n",
    "    idx = (-acquisition).argsort()[:n_instances]\n",
    "    query_idx = random_subset[idx]\n",
    "    return query_idx, X[query_idx]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list(input_list, name):\n",
    "    with open(\"perf_lists_paper/\" + name, 'w') as f:\n",
    "        for val in input_list:\n",
    "            f.write(\"%s\\n\" % val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active Learning Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_learning_procedure(query_strategy,\n",
    "                              X_test,\n",
    "                              y_test,\n",
    "                              X_pool,\n",
    "                              y_pool,\n",
    "                              X_initial,\n",
    "                              y_initial,\n",
    "                              estimator,\n",
    "                              n_queries=150,\n",
    "                              n_instances=10):\n",
    "    learner = ActiveLearner(estimator=estimator,\n",
    "                            X_training=X_initial,\n",
    "                            y_training=y_initial,\n",
    "                            query_strategy=query_strategy,\n",
    "                           )\n",
    "    perf_hist = [learner.score(X_test, y_test)]\n",
    "    X_rolling, y_rolling = np.copy(X_initial), np.copy(y_initial)\n",
    "    for index in range(n_queries):\n",
    "        query_idx, query_instance = learner.query(X_pool, n_instances)\n",
    "#         learner.teach(X_pool[query_idx], y_pool[query_idx])\n",
    "        X_rolling, y_rolling = np.concatenate((X_rolling, X_pool[query_idx]), axis=0), np.concatenate((y_rolling, y_pool[query_idx]))\n",
    "        learner.fit(X_rolling, y_rolling)\n",
    "        X_pool = np.delete(X_pool, query_idx, axis=0)\n",
    "        y_pool = np.delete(y_pool, query_idx, axis=0)\n",
    "        model_accuracy = learner.score(X_test, y_test)\n",
    "        print('Accuracy after query {n}: {acc:0.4f}'.format(n=index + 1, acc=model_accuracy))\n",
    "        perf_hist.append(model_accuracy)\n",
    "    return perf_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after query 1: 0.8854\n",
      "Accuracy after query 2: 0.8997\n",
      "Accuracy after query 3: 0.9195\n",
      "Accuracy after query 4: 0.9194\n",
      "Accuracy after query 5: 0.9263\n",
      "Accuracy after query 6: 0.9356\n",
      "Accuracy after query 7: 0.9346\n",
      "Accuracy after query 8: 0.9455\n",
      "Accuracy after query 9: 0.9530\n",
      "Accuracy after query 10: 0.9484\n",
      "Accuracy after query 11: 0.9537\n",
      "Accuracy after query 12: 0.9527\n",
      "Accuracy after query 13: 0.9609\n",
      "Accuracy after query 14: 0.9585\n",
      "Accuracy after query 15: 0.9635\n",
      "Accuracy after query 16: 0.9600\n",
      "Accuracy after query 17: 0.9651\n",
      "Accuracy after query 18: 0.9661\n",
      "Accuracy after query 19: 0.9685\n",
      "Accuracy after query 20: 0.9672\n",
      "Accuracy after query 21: 0.9660\n",
      "Accuracy after query 22: 0.9682\n",
      "Accuracy after query 23: 0.9690\n",
      "Accuracy after query 24: 0.9711\n",
      "Accuracy after query 25: 0.9733\n",
      "Accuracy after query 26: 0.9724\n",
      "Accuracy after query 27: 0.9706\n",
      "Accuracy after query 28: 0.9734\n",
      "Accuracy after query 29: 0.9716\n",
      "Accuracy after query 30: 0.9732\n",
      "Accuracy after query 31: 0.9751\n",
      "Accuracy after query 32: 0.9729\n",
      "Accuracy after query 33: 0.9759\n",
      "Accuracy after query 34: 0.9753\n",
      "Accuracy after query 35: 0.9740\n",
      "Accuracy after query 36: 0.9762\n",
      "Accuracy after query 37: 0.9760\n",
      "Accuracy after query 38: 0.9756\n",
      "Accuracy after query 39: 0.9763\n",
      "Accuracy after query 40: 0.9760\n",
      "Accuracy after query 41: 0.9761\n",
      "Accuracy after query 42: 0.9756\n",
      "Accuracy after query 43: 0.9788\n",
      "Accuracy after query 44: 0.9776\n",
      "Accuracy after query 45: 0.9762\n",
      "Accuracy after query 46: 0.9776\n",
      "Accuracy after query 47: 0.9783\n",
      "Accuracy after query 48: 0.9779\n",
      "Accuracy after query 49: 0.9791\n",
      "Accuracy after query 50: 0.9788\n",
      "Accuracy after query 51: 0.9771\n",
      "Accuracy after query 52: 0.9799\n",
      "Accuracy after query 53: 0.9830\n",
      "Accuracy after query 54: 0.9786\n",
      "Accuracy after query 55: 0.9799\n",
      "Accuracy after query 56: 0.9791\n",
      "Accuracy after query 57: 0.9795\n",
      "Accuracy after query 58: 0.9806\n",
      "Accuracy after query 59: 0.9800\n",
      "Accuracy after query 60: 0.9809\n",
      "Accuracy after query 61: 0.9814\n",
      "Accuracy after query 62: 0.9837\n",
      "Accuracy after query 63: 0.9832\n",
      "Accuracy after query 64: 0.9816\n",
      "Accuracy after query 65: 0.9822\n",
      "Accuracy after query 66: 0.9817\n",
      "Accuracy after query 67: 0.9829\n",
      "Accuracy after query 68: 0.9816\n",
      "Accuracy after query 69: 0.9818\n",
      "Accuracy after query 70: 0.9807\n",
      "Accuracy after query 71: 0.9825\n",
      "Accuracy after query 72: 0.9805\n",
      "Accuracy after query 73: 0.9818\n",
      "Accuracy after query 74: 0.9838\n",
      "Accuracy after query 75: 0.9809\n",
      "Accuracy after query 76: 0.9817\n",
      "Accuracy after query 77: 0.9836\n",
      "Accuracy after query 78: 0.9826\n",
      "Accuracy after query 79: 0.9810\n",
      "Accuracy after query 80: 0.9814\n",
      "Accuracy after query 81: 0.9825\n",
      "Accuracy after query 82: 0.9829\n",
      "Accuracy after query 83: 0.9849\n",
      "Accuracy after query 84: 0.9819\n",
      "Accuracy after query 85: 0.9821\n",
      "Accuracy after query 86: 0.9847\n",
      "Accuracy after query 87: 0.9844\n",
      "Accuracy after query 88: 0.9845\n",
      "Accuracy after query 89: 0.9869\n",
      "Accuracy after query 90: 0.9843\n",
      "Accuracy after query 91: 0.9857\n",
      "Accuracy after query 92: 0.9839\n",
      "Accuracy after query 93: 0.9847\n",
      "Accuracy after query 94: 0.9854\n",
      "Accuracy after query 95: 0.9866\n",
      "Accuracy after query 96: 0.9858\n",
      "Accuracy after query 97: 0.9840\n",
      "Accuracy after query 98: 0.9855\n",
      "Accuracy after query 99: 0.9839\n",
      "Accuracy after query 100: 0.9866\n",
      "Accuracy after query 101: 0.9842\n",
      "Accuracy after query 102: 0.9865\n",
      "Accuracy after query 103: 0.9863\n",
      "Accuracy after query 104: 0.9841\n",
      "Accuracy after query 105: 0.9868\n",
      "Accuracy after query 106: 0.9855\n",
      "Accuracy after query 107: 0.9867\n",
      "Accuracy after query 108: 0.9870\n",
      "Accuracy after query 109: 0.9849\n",
      "Accuracy after query 110: 0.9861\n",
      "Accuracy after query 111: 0.9872\n",
      "Accuracy after query 112: 0.9860\n",
      "Accuracy after query 113: 0.9863\n",
      "Accuracy after query 114: 0.9857\n",
      "Accuracy after query 115: 0.9862\n",
      "Accuracy after query 116: 0.9853\n",
      "Accuracy after query 117: 0.9881\n",
      "Accuracy after query 118: 0.9862\n",
      "Accuracy after query 119: 0.9851\n",
      "Accuracy after query 120: 0.9855\n",
      "Accuracy after query 121: 0.9876\n",
      "Accuracy after query 122: 0.9861\n",
      "Accuracy after query 123: 0.9866\n",
      "Accuracy after query 124: 0.9864\n",
      "Accuracy after query 125: 0.9877\n",
      "Accuracy after query 126: 0.9873\n",
      "Accuracy after query 127: 0.9863\n",
      "Accuracy after query 128: 0.9883\n",
      "Accuracy after query 129: 0.9884\n",
      "Accuracy after query 130: 0.9876\n",
      "Accuracy after query 131: 0.9873\n",
      "Accuracy after query 132: 0.9856\n",
      "Accuracy after query 133: 0.9869\n",
      "Accuracy after query 134: 0.9861\n",
      "Accuracy after query 135: 0.9862\n",
      "Accuracy after query 136: 0.9876\n",
      "Accuracy after query 137: 0.9888\n",
      "Accuracy after query 138: 0.9854\n",
      "Accuracy after query 139: 0.9876\n",
      "Accuracy after query 140: 0.9866\n",
      "Accuracy after query 141: 0.9880\n",
      "Accuracy after query 142: 0.9869\n",
      "Accuracy after query 143: 0.9877\n",
      "Accuracy after query 144: 0.9860\n",
      "Accuracy after query 145: 0.9861\n",
      "Accuracy after query 146: 0.9872\n",
      "Accuracy after query 147: 0.9857\n",
      "Accuracy after query 148: 0.9895\n",
      "Accuracy after query 149: 0.9898\n",
      "Accuracy after query 150: 0.9894\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# estimator = NeuralNetClassifier(lenet,\n",
    "#                                 max_epochs=50,\n",
    "#                                 batch_size=100,\n",
    "#                                 lr=0.01,\n",
    "#                                 optimizer=torch.optim.Adam,\n",
    "#                                 optimizer__betas=(0.5, 0.999),\n",
    "#                                 criterion=torch.nn.CrossEntropyLoss,\n",
    "#                                 train_split=None,\n",
    "#                                 verbose=0,\n",
    "#                                 device=device)\n",
    "# estimator = NeuralNetClassifier(lenet,\n",
    "#                                 max_epochs=50,\n",
    "#                                 batch_size=100,\n",
    "#                                 lr=0.001,\n",
    "#                                 optimizer=torch.optim.Adam,\n",
    "#                                 criterion=torch.nn.CrossEntropyLoss,\n",
    "#                                 train_split=None,\n",
    "#                                 verbose=0,\n",
    "#                                 device=device)\n",
    "estimator = NeuralNetClassifier(lenet,\n",
    "                                max_epochs=50,\n",
    "                                batch_size=100,\n",
    "                                lr=1.0,\n",
    "                                optimizer=torch.optim.Adadelta,\n",
    "                                optimizer__rho=0.9,\n",
    "                                optimizer__eps=1e-6,\n",
    "                                criterion=torch.nn.CrossEntropyLoss,\n",
    "                                train_split=None,\n",
    "                                verbose=0,\n",
    "                                device=device)\n",
    "uniform_perf_hist = active_learning_procedure(uniform,\n",
    "                                              X_test,\n",
    "                                              y_test,\n",
    "                                              X_pool,\n",
    "                                              y_pool,\n",
    "                                              X_initial,\n",
    "                                              y_initial,\n",
    "                                              estimator,\n",
    "                                             n_instances=100)\n",
    "save_list(uniform_perf_hist, \"uniform_perf_hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after query 1: 0.8531\n",
      "Accuracy after query 2: 0.9096\n",
      "Accuracy after query 3: 0.9332\n",
      "Accuracy after query 4: 0.9430\n",
      "Accuracy after query 5: 0.9499\n",
      "Accuracy after query 6: 0.9503\n",
      "Accuracy after query 7: 0.9567\n",
      "Accuracy after query 8: 0.9611\n",
      "Accuracy after query 9: 0.9668\n",
      "Accuracy after query 10: 0.9654\n",
      "Accuracy after query 11: 0.9703\n",
      "Accuracy after query 12: 0.9702\n",
      "Accuracy after query 13: 0.9738\n",
      "Accuracy after query 14: 0.9759\n",
      "Accuracy after query 15: 0.9769\n",
      "Accuracy after query 16: 0.9789\n",
      "Accuracy after query 17: 0.9775\n",
      "Accuracy after query 18: 0.9801\n",
      "Accuracy after query 19: 0.9796\n",
      "Accuracy after query 20: 0.9797\n",
      "Accuracy after query 21: 0.9821\n",
      "Accuracy after query 22: 0.9816\n",
      "Accuracy after query 23: 0.9830\n",
      "Accuracy after query 24: 0.9833\n",
      "Accuracy after query 25: 0.9846\n",
      "Accuracy after query 26: 0.9855\n",
      "Accuracy after query 27: 0.9858\n",
      "Accuracy after query 28: 0.9854\n",
      "Accuracy after query 29: 0.9860\n",
      "Accuracy after query 30: 0.9850\n",
      "Accuracy after query 31: 0.9848\n",
      "Accuracy after query 32: 0.9865\n",
      "Accuracy after query 33: 0.9860\n",
      "Accuracy after query 34: 0.9872\n",
      "Accuracy after query 35: 0.9861\n",
      "Accuracy after query 36: 0.9859\n",
      "Accuracy after query 37: 0.9869\n",
      "Accuracy after query 38: 0.9864\n",
      "Accuracy after query 39: 0.9901\n",
      "Accuracy after query 40: 0.9878\n",
      "Accuracy after query 41: 0.9890\n",
      "Accuracy after query 42: 0.9888\n",
      "Accuracy after query 43: 0.9885\n",
      "Accuracy after query 44: 0.9894\n",
      "Accuracy after query 45: 0.9886\n",
      "Accuracy after query 46: 0.9892\n",
      "Accuracy after query 47: 0.9881\n",
      "Accuracy after query 48: 0.9877\n",
      "Accuracy after query 49: 0.9878\n",
      "Accuracy after query 50: 0.9890\n",
      "Accuracy after query 51: 0.9886\n",
      "Accuracy after query 52: 0.9886\n",
      "Accuracy after query 53: 0.9892\n",
      "Accuracy after query 54: 0.9903\n",
      "Accuracy after query 55: 0.9889\n",
      "Accuracy after query 56: 0.9890\n",
      "Accuracy after query 57: 0.9896\n",
      "Accuracy after query 58: 0.9900\n",
      "Accuracy after query 59: 0.9892\n",
      "Accuracy after query 60: 0.9896\n",
      "Accuracy after query 61: 0.9893\n",
      "Accuracy after query 62: 0.9890\n",
      "Accuracy after query 63: 0.9890\n",
      "Accuracy after query 64: 0.9913\n",
      "Accuracy after query 65: 0.9891\n",
      "Accuracy after query 66: 0.9900\n",
      "Accuracy after query 67: 0.9915\n",
      "Accuracy after query 68: 0.9910\n",
      "Accuracy after query 69: 0.9892\n",
      "Accuracy after query 70: 0.9918\n",
      "Accuracy after query 71: 0.9906\n",
      "Accuracy after query 72: 0.9892\n",
      "Accuracy after query 73: 0.9916\n",
      "Accuracy after query 74: 0.9908\n",
      "Accuracy after query 75: 0.9904\n",
      "Accuracy after query 76: 0.9902\n",
      "Accuracy after query 77: 0.9905\n",
      "Accuracy after query 78: 0.9909\n",
      "Accuracy after query 79: 0.9913\n",
      "Accuracy after query 80: 0.9903\n",
      "Accuracy after query 81: 0.9887\n",
      "Accuracy after query 82: 0.9903\n",
      "Accuracy after query 83: 0.9908\n",
      "Accuracy after query 84: 0.9885\n",
      "Accuracy after query 85: 0.9910\n",
      "Accuracy after query 86: 0.9906\n",
      "Accuracy after query 87: 0.9898\n",
      "Accuracy after query 88: 0.9916\n",
      "Accuracy after query 89: 0.9908\n",
      "Accuracy after query 90: 0.9906\n",
      "Accuracy after query 91: 0.9900\n",
      "Accuracy after query 92: 0.9900\n",
      "Accuracy after query 93: 0.9896\n",
      "Accuracy after query 94: 0.9899\n",
      "Accuracy after query 95: 0.9900\n",
      "Accuracy after query 96: 0.9914\n",
      "Accuracy after query 97: 0.9901\n",
      "Accuracy after query 98: 0.9911\n",
      "Accuracy after query 99: 0.9922\n",
      "Accuracy after query 100: 0.9921\n",
      "Accuracy after query 101: 0.9921\n",
      "Accuracy after query 102: 0.9910\n",
      "Accuracy after query 103: 0.9920\n",
      "Accuracy after query 104: 0.9920\n",
      "Accuracy after query 105: 0.9911\n",
      "Accuracy after query 106: 0.9914\n",
      "Accuracy after query 107: 0.9917\n",
      "Accuracy after query 108: 0.9914\n",
      "Accuracy after query 109: 0.9909\n",
      "Accuracy after query 110: 0.9911\n",
      "Accuracy after query 111: 0.9901\n",
      "Accuracy after query 112: 0.9917\n",
      "Accuracy after query 113: 0.9911\n",
      "Accuracy after query 114: 0.9915\n",
      "Accuracy after query 115: 0.9913\n",
      "Accuracy after query 116: 0.9908\n",
      "Accuracy after query 117: 0.9911\n",
      "Accuracy after query 118: 0.9927\n",
      "Accuracy after query 119: 0.9918\n",
      "Accuracy after query 120: 0.9919\n",
      "Accuracy after query 121: 0.9916\n",
      "Accuracy after query 122: 0.9920\n",
      "Accuracy after query 123: 0.9909\n",
      "Accuracy after query 124: 0.9908\n",
      "Accuracy after query 125: 0.9918\n",
      "Accuracy after query 126: 0.9913\n",
      "Accuracy after query 127: 0.9930\n",
      "Accuracy after query 128: 0.9915\n",
      "Accuracy after query 129: 0.9907\n",
      "Accuracy after query 130: 0.9905\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# estimator = NeuralNetClassifier(ResNetMNIST,\n",
    "#                                 max_epochs=50,\n",
    "#                                 batch_size=128,\n",
    "#                                 lr=0.001,\n",
    "#                                 optimizer=torch.optim.Adam,\n",
    "#                                 criterion=torch.nn.CrossEntropyLoss,\n",
    "#                                 train_split=None,\n",
    "#                                 verbose=0,\n",
    "#                                 device=device)\n",
    "estimator = NeuralNetClassifier(lenet,\n",
    "                                max_epochs=50,\n",
    "                                batch_size=100,\n",
    "                                lr=1.0,\n",
    "                                optimizer=torch.optim.Adadelta,\n",
    "                                optimizer__rho=0.9,\n",
    "                                optimizer__eps=1e-6,\n",
    "                                criterion=torch.nn.CrossEntropyLoss,\n",
    "                                train_split=None,\n",
    "                                verbose=0,\n",
    "                                device=device)\n",
    "bald_perf_hist = active_learning_procedure(bald,\n",
    "                                           X_test,\n",
    "                                           y_test,\n",
    "                                           X_pool,\n",
    "                                           y_pool,\n",
    "                                           X_initial,\n",
    "                                           y_initial,\n",
    "                                           estimator,\n",
    "                                          n_instances=100)\n",
    "save_list(bald_perf_hist, \"bald_perf_hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# estimator = NeuralNetClassifier(ResNetMNIST,\n",
    "#                                 max_epochs=50,\n",
    "#                                 batch_size=128,\n",
    "#                                 lr=0.001,\n",
    "#                                 optimizer=torch.optim.Adam,\n",
    "#                                 criterion=torch.nn.CrossEntropyLoss,\n",
    "#                                 train_split=None,\n",
    "#                                 verbose=0,\n",
    "#                                 device=device)\n",
    "estimator = NeuralNetClassifier(lenet,\n",
    "                                max_epochs=50,\n",
    "                                batch_size=100,\n",
    "                                lr=1.0,\n",
    "                                optimizer=torch.optim.Adadelta,\n",
    "                                optimizer__rho=0.9,\n",
    "                                optimizer__eps=1e-6,\n",
    "                                criterion=torch.nn.CrossEntropyLoss,\n",
    "                                train_split=None,\n",
    "                                verbose=0,\n",
    "                                device=device)\n",
    "entropy_perf_hist = active_learning_procedure(max_entropy,\n",
    "                                              X_test,\n",
    "                                              y_test,\n",
    "                                              X_pool,\n",
    "                                              y_pool,\n",
    "                                              X_initial,\n",
    "                                              y_initial,\n",
    "                                              estimator,\n",
    "                                             n_instances=100)\n",
    "save_list(entropy_perf_hist, \"entropy_perf_hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p38)",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
