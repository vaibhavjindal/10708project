{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "podfwg3rfvhv"
      },
      "source": [
        "This is an implementation of the paper Deep Bayesian Active Learning with Image Data using PyTorch and modAL. \n",
        "\n",
        "modAL is an active learning framework for Python3, designed with modularity, flexibility and extensibility in mind. Built on top of scikit-learn, it allows you to rapidly create active learning workflows with nearly complete freedom. What is more, you can easily replace parts with your custom built solutions, allowing you to design novel algorithms with ease.\n",
        "\n",
        "Since modAL only supports sklearn models, we will also use [skorch](https://skorch.readthedocs.io/en/stable/), a scikit-learn compatible neural network library that wraps PyTorch. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U skorch\n",
        "!pip install modAL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51BvIpz4f4i3",
        "outputId": "7294c62b-3636-4536-e196-c5fa437edebb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: skorch in /usr/local/lib/python3.7/dist-packages (0.11.0)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.7/dist-packages (from skorch) (4.64.0)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from skorch) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from skorch) (1.4.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from skorch) (0.8.9)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from skorch) (1.21.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->skorch) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->skorch) (3.1.0)\n",
            "Requirement already satisfied: modAL in /usr/local/lib/python3.7/dist-packages (0.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from modAL) (1.0.2)\n",
            "Requirement already satisfied: scipy>=0.18 in /usr/local/lib/python3.7/dist-packages (from modAL) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.7/dist-packages (from modAL) (1.21.6)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from modAL) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->modAL) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->modAL) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1.0->modAL) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->modAL) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->modAL) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir perf_lists_cifar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPEvOqqwgopX",
        "outputId": "86b1f6cb-dd4e-4b43-f0ee-d4030e82ec5e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘perf_lists_cifar’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dkBpar9Ffvh2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.datasets import CIFAR10\n",
        "from skorch import NeuralNetClassifier\n",
        "from modAL.models import ActiveLearner\n",
        "from torchvision import transforms\n",
        "# from VAE import VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJS_vrlWfvh4"
      },
      "source": [
        "### architecture of the network we will be using\n",
        "\n",
        "We will use the architecture described in the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LdBnvn-Wfvh5"
      },
      "outputs": [],
      "source": [
        "# class CNN(nn.Module):\n",
        "#     def __init__(self,):\n",
        "#         super(CNN, self).__init__()\n",
        "#         self.convs = nn.Sequential(\n",
        "#                                 nn.Conv2d(1,32,4),\n",
        "#                                 nn.ReLU(),\n",
        "#                                 nn.Conv2d(32,32,4),\n",
        "#                                 nn.ReLU(),\n",
        "#                                 nn.MaxPool2d(2),\n",
        "#                                 nn.Dropout(0.25)\n",
        "#         )\n",
        "#         self.fcs = nn.Sequential(\n",
        "#                                 nn.Linear(11*11*32,128),\n",
        "#                                 nn.ReLU(),\n",
        "#                                 nn.Dropout(0.5),\n",
        "#                                 nn.Linear(128,10),\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         out = x\n",
        "#         out = self.convs(out)\n",
        "#         out = out.view(-1,11*11*32)\n",
        "#         out = self.fcs(out)\n",
        "#         return out\n",
        "\n",
        "input_dim, input_height, input_width = 3, 28, 28\n",
        "class lenet(nn.Module):  \n",
        "    def __init__(self):\n",
        "        super(lenet, self).__init__()\n",
        "        self.input_height = input_height\n",
        "        self.input_width = input_width\n",
        "        self.input_dim = input_dim\n",
        "        self.class_num = 10\n",
        "\n",
        "        self.conv1 = nn.Conv2d(self.input_dim, 6, (5, 5), padding=2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, (5, 5))\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, self.class_num)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "# class CNN(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "#         self.pool = nn.MaxPool2d(2, 2)\n",
        "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "#         self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "#         self.fc2 = nn.Linear(120, 84)\n",
        "#         self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.pool(F.relu(self.conv1(x)))\n",
        "#         x = self.pool(F.relu(self.conv2(x)))\n",
        "#         x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         x = F.relu(self.fc2(x))\n",
        "#         x = self.fc3(x)\n",
        "#         return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JKD8nilfvh6"
      },
      "source": [
        "### read training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_SzuFiCfvh7",
        "outputId": "a5f28d70-95e9-4a34-d583-6042720e4b1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "cifar10_train = CIFAR10('.', train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Resize(size=(28, 28))]))\n",
        "cifar10_test  = CIFAR10('.', train=False,download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Resize(size=(28, 28))]))\n",
        "traindataloader = DataLoader(cifar10_train, shuffle=True, batch_size=60000)\n",
        "testdataloader  = DataLoader(cifar10_train, shuffle=True, batch_size=10000)\n",
        "X_train, y_train = next(iter(traindataloader))\n",
        "X_test , y_test  = next(iter(testdataloader))\n",
        "X_train, y_train = X_train.detach().cpu().numpy(), y_train.detach().cpu().numpy()\n",
        "X_test, y_test = X_test.detach().cpu().numpy(), y_test.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGSELhG-fvh8",
        "outputId": "98d7df45-cc63-470e-e5ac-8bffc06aef65"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 3, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtuf6UD8fvh9"
      },
      "source": [
        "### preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ydKf--safvh-"
      },
      "outputs": [],
      "source": [
        "# X_train = X_train.reshape(60000, 1, 28, 28)\n",
        "# X_test = X_test.reshape(10000, 1, 28, 28)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMzkFce5fvh-"
      },
      "source": [
        "### initial labelled data\n",
        "We initialize the labelled set with 100 balanced randomly sampled examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Rm7FGWpdfvh_"
      },
      "outputs": [],
      "source": [
        "initial_idx = np.array([],dtype=int)\n",
        "for i in range(10):\n",
        "    idx = np.random.choice(np.where(y_train==i)[0], size=10, replace=False)\n",
        "    initial_idx = np.concatenate((initial_idx, idx))\n",
        "\n",
        "X_initial = X_train[initial_idx]\n",
        "y_initial = y_train[initial_idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3MDgai1fviA"
      },
      "source": [
        "### initial unlabelled pool"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X_pool = np.delete(X_train, initial_idx, axis=0)\n",
        "# y_pool = np.delete(y_train, initial_idx, axis=0)\n",
        "X_pool = np.copy(X_train)\n",
        "y_pool = np.copy(y_train)"
      ],
      "metadata": {
        "id": "Jn614AyZgbjG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZBr5B3BfviA"
      },
      "source": [
        "## Query Strategies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rzv-38YCfviB"
      },
      "source": [
        "### Uniform\n",
        "All the acquisition function we will use will be compared to the uniform acquisition function $\\mathbb{U}_{[0,1]}$ which will be our baseline that we would like to beat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "krYLyTnhfviB"
      },
      "outputs": [],
      "source": [
        "def uniform(learner, X, n_instances=1):\n",
        "    query_idx = np.random.choice(range(len(X)), size=n_instances, replace=False)\n",
        "    return query_idx, X[query_idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGPs48_BfviB"
      },
      "source": [
        "### Entropy\n",
        "Our first acquisition function is the entropy:\n",
        "$$ \\mathbb{H} = - \\sum_{c} p_c \\log(p_c)$$\n",
        "where $p_c$ is the probability predicted for class c. This is approximated by:\n",
        "\\begin{align}\n",
        "p_c &= \\frac{1}{T} \\sum_t p_{c}^{(t)} \n",
        "\\end{align}\n",
        "where $p_{c}^{t}$ is the probability predicted for class c at the t th feedforward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "G3E0tMppfviC"
      },
      "outputs": [],
      "source": [
        "def max_entropy(learner, X, n_instances=1, T=100):\n",
        "    random_subset = np.random.choice(range(len(X)), size=2000, replace=False)\n",
        "    with torch.no_grad():\n",
        "        outputs = np.stack([torch.softmax(learner.estimator.forward(X[random_subset], training=True),dim=-1).cpu().numpy()\n",
        "                            for t in range(20)])\n",
        "    pc = outputs.mean(axis=0)\n",
        "    acquisition = (-pc*np.log(pc + 1e-10)).sum(axis=-1)\n",
        "    idx = (-acquisition).argsort()[:n_instances]\n",
        "    query_idx = random_subset[idx]\n",
        "    return query_idx, X[query_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "E00CVe7ffviC"
      },
      "outputs": [],
      "source": [
        "def bald(learner, X, n_instances=1, T=100):\n",
        "    random_subset = np.random.choice(range(len(X)), size=2000, replace=False)\n",
        "    with torch.no_grad():\n",
        "        outputs = np.stack([torch.softmax(learner.estimator.forward(X[random_subset], training=True),dim=-1).cpu().numpy()\n",
        "                            for t in range(20)])\n",
        "    pc = outputs.mean(axis=0)\n",
        "    H   = (-pc*np.log(pc + 1e-10)).sum(axis=-1)\n",
        "    E_H = - np.mean(np.sum(outputs * np.log(outputs + 1e-10), axis=-1), axis=0)  # [batch size]\n",
        "    acquisition = H - E_H\n",
        "    idx = (-acquisition).argsort()[:n_instances]\n",
        "    query_idx = random_subset[idx]\n",
        "    return query_idx, X[query_idx]    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3qe2UZG5fviC"
      },
      "outputs": [],
      "source": [
        "def save_list(input_list, name):\n",
        "    with open(\"perf_lists_cifar/\" + name, 'w') as f:\n",
        "        for val in input_list:\n",
        "            f.write(\"%s\\n\" % val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeAVxYAXfviD"
      },
      "source": [
        "### Active Learning Procedure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "osxuqPxFfviD"
      },
      "outputs": [],
      "source": [
        "def active_learning_procedure(query_strategy,\n",
        "                              X_test,\n",
        "                              y_test,\n",
        "                              X_pool,\n",
        "                              y_pool,\n",
        "                              X_initial,\n",
        "                              y_initial,\n",
        "                              estimator,\n",
        "                              n_queries=150,\n",
        "                              n_instances=100):\n",
        "    learner = ActiveLearner(estimator=estimator,\n",
        "                            X_training=X_initial,\n",
        "                            y_training=y_initial,\n",
        "                            query_strategy=query_strategy,\n",
        "                           )\n",
        "    perf_hist = [learner.score(X_test, y_test)]\n",
        "    X_rolling, y_rolling = np.copy(X_initial), np.copy(y_initial)\n",
        "    for index in range(n_queries):\n",
        "        query_idx, query_instance = learner.query(X_pool, n_instances)\n",
        "#         learner.teach(X_pool[query_idx], y_pool[query_idx])\n",
        "        X_rolling, y_rolling = np.concatenate((X_rolling, X_pool[query_idx]), axis=0), np.concatenate((y_rolling, y_pool[query_idx]))\n",
        "        learner.fit(X_rolling, y_rolling)\n",
        "        X_pool = np.delete(X_pool, query_idx, axis=0)\n",
        "        y_pool = np.delete(y_pool, query_idx, axis=0)\n",
        "        model_accuracy = learner.score(X_test, y_test)\n",
        "        print('Accuracy after query {n}: {acc:0.4f}'.format(n=index + 1, acc=model_accuracy))\n",
        "        perf_hist.append(model_accuracy)\n",
        "    return perf_hist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7eGlDIKgg73H"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "It1GE2nqfviD",
        "outputId": "c939755b-192f-47c1-c9b5-a939ce04abe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy after query 1: 0.2549\n",
            "Accuracy after query 2: 0.2146\n",
            "Accuracy after query 3: 0.2559\n",
            "Accuracy after query 4: 0.2643\n",
            "Accuracy after query 5: 0.2598\n",
            "Accuracy after query 6: 0.3485\n",
            "Accuracy after query 7: 0.3684\n",
            "Accuracy after query 8: 0.3562\n",
            "Accuracy after query 9: 0.3692\n",
            "Accuracy after query 10: 0.3681\n",
            "Accuracy after query 11: 0.3922\n",
            "Accuracy after query 12: 0.3644\n",
            "Accuracy after query 13: 0.3548\n",
            "Accuracy after query 14: 0.3710\n",
            "Accuracy after query 15: 0.4004\n",
            "Accuracy after query 16: 0.4164\n",
            "Accuracy after query 17: 0.4032\n",
            "Accuracy after query 18: 0.4091\n",
            "Accuracy after query 19: 0.4285\n",
            "Accuracy after query 20: 0.4146\n",
            "Accuracy after query 21: 0.4385\n",
            "Accuracy after query 22: 0.4516\n",
            "Accuracy after query 23: 0.4440\n",
            "Accuracy after query 24: 0.4002\n",
            "Accuracy after query 25: 0.4354\n",
            "Accuracy after query 26: 0.4457\n",
            "Accuracy after query 27: 0.4184\n",
            "Accuracy after query 28: 0.4587\n",
            "Accuracy after query 29: 0.4610\n",
            "Accuracy after query 30: 0.4599\n",
            "Accuracy after query 31: 0.4758\n",
            "Accuracy after query 32: 0.4696\n",
            "Accuracy after query 33: 0.4692\n",
            "Accuracy after query 34: 0.4737\n",
            "Accuracy after query 35: 0.4733\n",
            "Accuracy after query 36: 0.4614\n",
            "Accuracy after query 37: 0.4483\n",
            "Accuracy after query 38: 0.4777\n",
            "Accuracy after query 39: 0.4613\n",
            "Accuracy after query 40: 0.4204\n",
            "Accuracy after query 41: 0.4857\n",
            "Accuracy after query 42: 0.4914\n",
            "Accuracy after query 43: 0.4732\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "estimator = NeuralNetClassifier(lenet,\n",
        "                                max_epochs=50,\n",
        "                                batch_size=100,\n",
        "                                lr=1.0,\n",
        "                                optimizer=torch.optim.Adadelta,\n",
        "                                optimizer__rho=0.9,\n",
        "                                optimizer__eps=1e-6,\n",
        "                                criterion=torch.nn.CrossEntropyLoss,\n",
        "                                train_split=None,\n",
        "                                verbose=0,\n",
        "                                device=device)\n",
        "uniform_perf_hist = active_learning_procedure(uniform,\n",
        "                                              X_test,\n",
        "                                              y_test,\n",
        "                                              X_pool,\n",
        "                                              y_pool,\n",
        "                                              X_initial,\n",
        "                                              y_initial,\n",
        "                                              estimator,\n",
        "                                             n_instances=100)\n",
        "save_list(uniform_perf_hist, \"uniform_perf_hist\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmEljLvGfviE"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "estimator = NeuralNetClassifier(lenet,\n",
        "                                max_epochs=50,\n",
        "                                batch_size=100,\n",
        "                                lr=1.0,\n",
        "                                optimizer=torch.optim.Adadelta,\n",
        "                                optimizer__rho=0.9,\n",
        "                                optimizer__eps=1e-6,\n",
        "                                criterion=torch.nn.CrossEntropyLoss,\n",
        "                                train_split=None,\n",
        "                                verbose=0,\n",
        "                                device=device)\n",
        "bald_perf_hist = active_learning_procedure(bald,\n",
        "                                              X_test,\n",
        "                                              y_test,\n",
        "                                              X_pool,\n",
        "                                              y_pool,\n",
        "                                              X_initial,\n",
        "                                              y_initial,\n",
        "                                              estimator,\n",
        "                                             n_instances=100)\n",
        "save_list(bald_perf_hist, \"bald_perf_hist\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U820s4q2fviF"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "estimator = NeuralNetClassifier(lenet,\n",
        "                                max_epochs=50,\n",
        "                                batch_size=100,\n",
        "                                lr=1.0,\n",
        "                                optimizer=torch.optim.Adadelta,\n",
        "                                optimizer__rho=0.9,\n",
        "                                optimizer__eps=1e-6,\n",
        "                                criterion=torch.nn.CrossEntropyLoss,\n",
        "                                train_split=None,\n",
        "                                verbose=0,\n",
        "                                device=device)\n",
        "entropy_perf_hist = active_learning_procedure(max_entropy,\n",
        "                                              X_test,\n",
        "                                              y_test,\n",
        "                                              X_pool,\n",
        "                                              y_pool,\n",
        "                                              X_initial,\n",
        "                                              y_initial,\n",
        "                                              estimator,\n",
        "                                             n_instances=100)\n",
        "save_list(entropy_perf_hist, \"entropy_perf_hist\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Za7keYZKfviF"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "dbal_pytorch_paper_cifar.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}